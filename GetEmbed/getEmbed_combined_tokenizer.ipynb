{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "import pprint\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertConfig, BertLMHeadModel\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from astTokenizer import CustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(n1, n2):\n",
    "    \"\"\"\n",
    "    takes 2 numpy array\n",
    "    n1, n2, each a vector embedding for a single code_string \n",
    "    \"\"\"\n",
    "    euclidean_dist=np.linalg.norm(n1-n2)\n",
    "    #print(n1==n2)\n",
    "    manhattan_dist=np.sum(np.abs(n1 - n2))\n",
    "    cosine_sim=np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
    "    dot_product=np.dot(n1,n2)\n",
    "\n",
    "    return euclidean_dist, manhattan_dist, cosine_sim, dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "edistances = []\n",
    "cosSims = []\n",
    "dotPros = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer.from_file('./combined_tokenizer')\n",
    "custom_t=CustomTokenizer(tokenizer)\n",
    "#fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=custom_t)\n",
    "#fast_tokenizer.mask_token='<mask>'\n",
    "#fast_tokenizer.pad_token='<pad>'\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertLMHeadModel.from_pretrained('../ast_transformer/checkpoint-12400/',output_hidden_states=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scd-88 data\n",
    "with open('../data/all.jsonl','r') as test_file:\n",
    "    test_list = list(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=json.loads(test_list[1*130])['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(custom_t.encode(code)[:512]))\n",
    "print(type(torch.tensor((custom_t.encode(code)[:512]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomTokenizer' object has no attribute 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fast_tokenizer\u001b[39m.\u001b[39;49mencode(code)\n",
      "File \u001b[0;32m~/Documents/SoftwareProject/HF/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2294\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2257\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2258\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2259\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2277\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   2278\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   2279\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2280\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2294\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2295\u001b[0m         text,\n\u001b[1;32m   2296\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2297\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2298\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2299\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2300\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2301\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2302\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2303\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2304\u001b[0m     )\n\u001b[1;32m   2306\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/SoftwareProject/HF/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2694\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2695\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2700\u001b[0m )\n\u001b[0;32m-> 2702\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2703\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2704\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2705\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2706\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2707\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2708\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2709\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2710\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2711\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2712\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2713\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2714\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2715\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2716\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2717\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2718\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2719\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2720\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2721\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/SoftwareProject/HF/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    503\u001b[0m         batched_input,\n\u001b[1;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    509\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    519\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/Documents/SoftwareProject/HF/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:421\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_text_or_text_pairs has to be a list or a tuple (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(batch_text_or_text_pairs)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    424\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    425\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[1;32m    429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SoftwareProject/HF/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:351\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.set_truncation_and_padding\u001b[0;34m(self, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_truncation_and_padding\u001b[39m(\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    324\u001b[0m     padding_strategy: PaddingStrategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m     pad_to_multiple_of: Optional[\u001b[39mint\u001b[39m],\n\u001b[1;32m    329\u001b[0m ):\n\u001b[1;32m    330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m    Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    library) and restore the tokenizer settings afterwards.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39m            the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     _truncation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mtruncation\n\u001b[1;32m    352\u001b[0m     _padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mpadding\n\u001b[1;32m    353\u001b[0m     \u001b[39m# Set truncation and padding on the backend tokenizer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomTokenizer' object has no attribute 'truncation'"
     ]
    }
   ],
   "source": [
    "fast_tokenizer.encode(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclids=list()\n",
    "cosines=list()\n",
    "dots=list()\n",
    "manhattans=list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(0,88,1):\n",
    "        pairs=list()\n",
    "\n",
    "        p_euclid=list()\n",
    "        p_cosine=list()\n",
    "        p_dot=list()\n",
    "        p_manhattan=list()\n",
    "\n",
    "        for p in range(2,102,1):\n",
    "            clone1 = json.loads(test_list[i*130])['code']\n",
    "            clone2 = json.loads(test_list[i*130+p])['code']\n",
    "\n",
    "            pairs.append([clone1, clone2])\n",
    "\n",
    "            #print(clone1)\n",
    "            #print(clone2)\n",
    "            #print('---------')\n",
    "\n",
    "        # get embedding\n",
    "        for code_pair in pairs:\n",
    "            code1 = code_pair[0]\n",
    "            code2 = code_pair[1]\n",
    "        \n",
    "            # Tokenize the input sentence\n",
    "            #id1 = fast_tokenizer.encode(code1, return_tensors='pt', max_length=512, truncation=True)\n",
    "            #id2 = fast_tokenizer.encode(code2, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "            id1= torch.tensor((custom_t.encode(code1)[:512]))\n",
    "            id2= torch.tensor((custom_t.encode(code2)[:512]))\n",
    "\n",
    "            # unsqueeze is necessary as our custom_tokenizer returns different shaped lists\n",
    "            # this converts [dim] to [1,dim] for single peace of code, which we are using here \n",
    "            id1=id1.unsqueeze(0)\n",
    "            id2=id2.unsqueeze(0)\n",
    "\n",
    "            #print(id1)\n",
    "            #print(type(id1))\n",
    "            #print(id1.shape)\n",
    "            \n",
    "            out_clone1 = model(id1)\n",
    "            out_clone2 = model(id2)\n",
    "\n",
    "            v1=torch.zeros(out_clone1.hidden_states[0].shape)# v1, v2 must match the shape\n",
    "            v2=torch.zeros(out_clone2.hidden_states[0].shape)\n",
    "\n",
    "            for i in range(len(out_clone1.hidden_states)):# we have 3 layers\n",
    "                v1 += out_clone1.hidden_states[i]\n",
    "            \n",
    "            mean_embed_clone1 = torch.mean(v1, dim=1).squeeze()\n",
    "\n",
    "            for i in range(len(out_clone2.hidden_states)):# we have 3 layers\n",
    "                v2 += out_clone2.hidden_states[i]\n",
    "            \n",
    "            mean_embed_clone2 = torch.mean(v2, dim=1).squeeze()\n",
    "            #print(mean_embed_clone2.shape)\n",
    "\n",
    "            e,m,c,d = calculate_distance(mean_embed_clone1.numpy(), mean_embed_clone2.numpy())\n",
    "            #print(m)\n",
    "            p_euclid.append(e)\n",
    "            p_cosine.append(c)\n",
    "            p_dot.append(d)\n",
    "            p_manhattan.append(m)\n",
    "\n",
    "        euclids.append(p_euclid)\n",
    "        cosines.append(p_cosine)\n",
    "        dots.append(p_dot)\n",
    "        manhattans.append(p_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([13005,  5971, 36646,  5971, 27207, 20429, 11172, 16228, 44446, 16228,\n",
       "         25702,  5971, 20260, 16228, 30587, 39189, 11065, 16228, 30587, 30587,\n",
       "         30587, 45933, 31975, 13005, 31045, 36646,  8995, 45933, 45932, 45932,\n",
       "         45932, 45932, 22454, 16228, 27207, 30587, 45933, 12796, 13005, 31045,\n",
       "         27207,  8995, 45933, 45932, 45932, 45932, 45932, 22454, 16228, 36646,\n",
       "         30587, 45933, 30834,  8995, 45933, 45932, 45932, 45932, 45932, 22454,\n",
       "         16228, 13005, 30587])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('euclids_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(euclids, f)\n",
    "\n",
    "with open('cosines_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(cosines, f)\n",
    "\n",
    "with open('dots_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(dots, f)\n",
    "\n",
    "with open('manhattans_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(manhattans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./euclids_only_nlp.pkl', 'rb') as f:\n",
    "#    l = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.884398,\n",
       " 6.215089,\n",
       " 6.360911,\n",
       " 4.2223067,\n",
       " 2.23794,\n",
       " 3.1225078,\n",
       " 6.3637614,\n",
       " 1.496454,\n",
       " 4.547542,\n",
       " 2.7679667,\n",
       " 3.6243298,\n",
       " 4.7919645,\n",
       " 0.42771906,\n",
       " 1.3792201,\n",
       " 6.3637295,\n",
       " 0.4023666,\n",
       " 5.1661496,\n",
       " 1.6584011,\n",
       " 6.3637295,\n",
       " 5.6052604,\n",
       " 1.1837924,\n",
       " 0.4023666,\n",
       " 1.0936736,\n",
       " 3.352812,\n",
       " 3.3447886,\n",
       " 5.383455,\n",
       " 3.660757,\n",
       " 1.1380686,\n",
       " 3.8493125,\n",
       " 0.34403887,\n",
       " 2.6193473,\n",
       " 2.0264428,\n",
       " 3.8138385,\n",
       " 0.44905597,\n",
       " 3.3216221,\n",
       " 0.4023666,\n",
       " 3.748607,\n",
       " 1.0827117,\n",
       " 1.793426,\n",
       " 0.34775072,\n",
       " 2.5879323,\n",
       " 0.9800423,\n",
       " 2.3634524,\n",
       " 2.8117063,\n",
       " 0.9666655,\n",
       " 2.3972418,\n",
       " 1.1111333,\n",
       " 0.5995409,\n",
       " 0.40684047,\n",
       " 3.765724,\n",
       " 3.4243615,\n",
       " 6.103532,\n",
       " 0.42210433,\n",
       " 1.142812,\n",
       " 1.9817126,\n",
       " 2.6780472,\n",
       " 0.4870172,\n",
       " 2.7623537,\n",
       " 3.2454026,\n",
       " 6.0420065,\n",
       " 3.2321534,\n",
       " 1.6505609,\n",
       " 1.4975889,\n",
       " 7.062634,\n",
       " 0.5406142,\n",
       " 3.117426,\n",
       " 3.3650715,\n",
       " 0.7964936,\n",
       " 0.9666655,\n",
       " 6.500004,\n",
       " 5.41262,\n",
       " 5.956803,\n",
       " 1.1075712,\n",
       " 0.42210433,\n",
       " 6.3545375,\n",
       " 0.42210433,\n",
       " 3.2946463,\n",
       " 4.6021156,\n",
       " 6.1708994,\n",
       " 2.7170625,\n",
       " 1.3001966,\n",
       " 0.5938603,\n",
       " 5.3299556,\n",
       " 2.4416337,\n",
       " 3.883006,\n",
       " 0.4870172,\n",
       " 0.4023666,\n",
       " 2.1888795,\n",
       " 0.42210433,\n",
       " 2.6321275,\n",
       " 4.3218594,\n",
       " 2.809766,\n",
       " 4.814258,\n",
       " 4.6803474,\n",
       " 4.658465,\n",
       " 1.0827117,\n",
       " 3.5182872,\n",
       " 5.2193313,\n",
       " 0.42210433,\n",
       " 1.9505343]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclids[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(euclids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
