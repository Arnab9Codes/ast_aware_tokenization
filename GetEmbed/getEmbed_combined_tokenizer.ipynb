{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aktarafder/Documents/SoftwareProject/HF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "import pprint\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertConfig, BertLMHeadModel\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from astTokenizer import CustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(n1, n2):\n",
    "    \"\"\"\n",
    "    takes 2 numpy array\n",
    "    n1, n2, each a vector embedding for a single code_string \n",
    "    \"\"\"\n",
    "    euclidean_dist=np.linalg.norm(n1-n2)\n",
    "    #print(n1==n2)\n",
    "    manhattan_dist=np.sum(np.abs(n1 - n2))\n",
    "    cosine_sim=np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
    "    dot_product=np.dot(n1,n2)\n",
    "\n",
    "    return euclidean_dist, manhattan_dist, cosine_sim, dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edistances = []\n",
    "cosSims = []\n",
    "dotPros = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer.from_file('./combined_tokenizer')\n",
    "custom_t=CustomTokenizer(tokenizer)\n",
    "#fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=custom_t)\n",
    "#fast_tokenizer.mask_token='<mask>'\n",
    "#fast_tokenizer.pad_token='<pad>'\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertLMHeadModel.from_pretrained('../ast_transformer/checkpoint-12400/',output_hidden_states=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scd-88 data\n",
    "with open('../data/all.jsonl','r') as test_file:\n",
    "    test_list = list(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=json.loads(test_list[1*130])['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(custom_t.encode(code)[:512]))\n",
    "print(type(torch.tensor((custom_t.encode(code)[:512]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclids=list()\n",
    "cosines=list()\n",
    "dots=list()\n",
    "manhattans=list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(0,88,1):\n",
    "        pairs=list()\n",
    "\n",
    "        p_euclid=list()\n",
    "        p_cosine=list()\n",
    "        p_dot=list()\n",
    "        p_manhattan=list()\n",
    "\n",
    "        for p in range(2,102,1):\n",
    "            clone1 = json.loads(test_list[i*130])['code']\n",
    "            clone2 = json.loads(test_list[i*130+p])['code']\n",
    "\n",
    "            pairs.append([clone1, clone2])\n",
    "\n",
    "            #print(clone1)\n",
    "            #print(clone2)\n",
    "            #print('---------')\n",
    "\n",
    "        # get embedding\n",
    "        for code_pair in pairs:\n",
    "            code1 = code_pair[0]\n",
    "            code2 = code_pair[1]\n",
    "        \n",
    "            # Tokenize the input sentence\n",
    "            #id1 = fast_tokenizer.encode(code1, return_tensors='pt', max_length=512, truncation=True)\n",
    "            #id2 = fast_tokenizer.encode(code2, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "            id1= torch.tensor((custom_t.encode(code1)[:512]))\n",
    "            id2= torch.tensor((custom_t.encode(code2)[:512]))\n",
    "\n",
    "            # unsqueeze is necessary as our custom_tokenizer returns different shaped lists\n",
    "            # this converts [dim] to [1,dim] for single peace of code, which we are using here \n",
    "            id1=id1.unsqueeze(0)\n",
    "            id2=id2.unsqueeze(0)\n",
    "\n",
    "            #print(id1)\n",
    "            #print(type(id1))\n",
    "            #print(id1.shape)\n",
    "            \n",
    "            out_clone1 = model(id1)\n",
    "            out_clone2 = model(id2)\n",
    "\n",
    "            v1=torch.zeros(out_clone1.hidden_states[0].shape)# v1, v2 must match the shape\n",
    "            v2=torch.zeros(out_clone2.hidden_states[0].shape)\n",
    "\n",
    "            for i in range(len(out_clone1.hidden_states)):# we have 3 layers\n",
    "                v1 += out_clone1.hidden_states[i]\n",
    "            \n",
    "            mean_embed_clone1 = torch.mean(v1, dim=1).squeeze()\n",
    "\n",
    "            for i in range(len(out_clone2.hidden_states)):# we have 3 layers\n",
    "                v2 += out_clone2.hidden_states[i]\n",
    "            \n",
    "            mean_embed_clone2 = torch.mean(v2, dim=1).squeeze()\n",
    "            #print(mean_embed_clone2.shape)\n",
    "\n",
    "            e,m,c,d = calculate_distance(mean_embed_clone1.numpy(), mean_embed_clone2.numpy())\n",
    "            #print(m)\n",
    "            p_euclid.append(e)\n",
    "            p_cosine.append(c)\n",
    "            p_dot.append(d)\n",
    "            p_manhattan.append(m)\n",
    "\n",
    "        euclids.append(p_euclid)\n",
    "        cosines.append(p_cosine)\n",
    "        dots.append(p_dot)\n",
    "        manhattans.append(p_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([13005,  5971, 36646,  5971, 27207, 20429, 11172, 16228, 44446, 16228,\n",
       "         25702,  5971, 20260, 16228, 30587, 39189, 11065, 16228, 30587, 30587,\n",
       "         30587, 45933, 31975, 13005, 31045, 36646,  8995, 45933, 45932, 45932,\n",
       "         45932, 45932, 22454, 16228, 27207, 30587, 45933, 12796, 13005, 31045,\n",
       "         27207,  8995, 45933, 45932, 45932, 45932, 45932, 22454, 16228, 36646,\n",
       "         30587, 45933, 30834,  8995, 45933, 45932, 45932, 45932, 45932, 22454,\n",
       "         16228, 13005, 30587])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('euclids_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(euclids, f)\n",
    "\n",
    "with open('cosines_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(cosines, f)\n",
    "\n",
    "with open('dots_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(dots, f)\n",
    "\n",
    "with open('manhattans_combined.pkl', 'wb') as f:\n",
    "    pickle.dump(manhattans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
