{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "\n",
    "Language.build_library(\n",
    "  # Store the library in the `build` directory\n",
    "  'build/my-languages.so',\n",
    "\n",
    "  # Include one or more languages\n",
    "  [\n",
    "    #'Somenotebooks/tsitter/tree-sitter-python'\n",
    "    #'/home/aktarafder/Documents/SoftwareProject/HF/Somenotebooks/tsitter/tree-sitter-python',\n",
    "    './tree-sitter-python'\n",
    "    #'/home/aktarafder/Documents/SoftwareProject/HF/Somenotebooks/tsitter/tree-sitter-java'\n",
    "  ]\n",
    ")\n",
    "PYTHON_LANGUAGE = Language('build/my-languages.so', 'python')\n",
    "parser = Parser()\n",
    "parser.set_language(PYTHON_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset code_search_net (/home/aktarafder/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 171.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, Split\n",
    "import json\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names,get_dataset_config_names\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import BertConfig, BertLMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import tree_sitter # parsing library\n",
    "from tree_sitter import Language, Parser\n",
    "from datasets import load_dataset\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# loading codesearch net java dataset\n",
    "dataset_codeSearch_python=load_dataset(\"code_search_net\",\"python\")\n",
    "dpython=dataset_codeSearch_python\n",
    "print(\"loaded the dataset.\")\n",
    "unk_token = \"<UNK>\"  # token for unknown words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build an initial corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_from_ast=[]\n",
    "\n",
    "with open('./TXT_Files/tokens1to25k.txt','r') as f:\n",
    "    contents=f.readlines()\n",
    "all_tokens_from_ast = [line.strip() for line in contents]\n",
    "token_set=set(all_tokens_from_ast)\n",
    "all_tokens_from_ast.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreq(token_list):\n",
    "    word_freq=defaultdict(int)\n",
    "    for token in token_list:\n",
    "        word_freq[token]+=1\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def topK(word_freq,K):\n",
    "    s=sorted(word_freq.items(), key= lambda x:x[1], reverse=True)\n",
    "    return defaultdict(int,s[:K]) # assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr=getFreq(all_tokens_from_ast)\n",
    "s=topK(fr,20000)\n",
    "vocab = list()\n",
    "vocab.extend(s.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPFreq(splits):\n",
    "\n",
    "    pair_freqs=defaultdict(int)\n",
    "    for j in range(len(splits)-1):\n",
    "        pair=(splits[j], splits[j+1])\n",
    "        if (splits[j].startswith('#') or splits[j+1].startswith('#')):\n",
    "            continue\n",
    "        else:\n",
    "            pair_freqs[pair]+=1\n",
    "\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freq=calPFreq(all_tokens_from_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', '.') 52926\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freq.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    #for word in splits:\n",
    "    i = 0\n",
    "    while i < len(splits) - 1:\n",
    "        if splits[i] == a and splits[i + 1] == b:\n",
    "            if len(a)==1 or len(b)==1:\n",
    "                splits = splits[:i] + [a + b] + splits[i + 2 :]\n",
    "            else:\n",
    "                splits = splits[:i] + [a +\" \"+b] + splits[i + 2 :]\n",
    "        else:\n",
    "            i += 1\n",
    "    #splits[word] = splits\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list=all_tokens_from_ast[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=merge_pair('self','.', token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_size = 2000\n",
    "\n",
    "splits=all_tokens_from_ast[:30000]\n",
    "merges=dict()\n",
    "for i in range(merge_size):\n",
    "    pair_freqs = calPFreq(splits)\n",
    "    #print(pair_freqs)\n",
    "    best_pair = next(iter(pair_freq))# just selecting the first one, so that no error shows later\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if (max_freq is None or max_freq < freq):\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    #print('best pair:',best_pair)\n",
    "    if max_freq>2:\n",
    "        splits = merge_pair(best_pair[0], best_pair[1], splits)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        vocab.append(best_pair[0] + best_pair[1])\n",
    "        #print(best_pair[0]+best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22796"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignEmbed(vocab):\n",
    "    uniqId=0\n",
    "    BPE=dict()\n",
    "    for i in range(len(vocab)):\n",
    "        BPE[vocab[i]]=uniqId\n",
    "        uniqId=uniqId+1\n",
    "    \n",
    "    return BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe=assignEmbed(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "#merged_tokens=[]\n",
    "def tokenizeSinlge(i):\n",
    "    code_string=dpython['train']['whole_func_string'][i]\n",
    "    split_text = re.split(r'(\\s+)', code_string)\n",
    "\n",
    "    #print(split_text)\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(split_text):\n",
    "            j = 0\n",
    "            while j < len(split) - 1:\n",
    "                if split[j] == pair[0] and split[j + 1] == pair[1]:\n",
    "                    #print(\"merge:\", merge)\n",
    "                    split_text= split_text[:j] + [merge] + split_text[j + 2 :]\n",
    "                    \n",
    "                j += 1\n",
    "        # I need to have a dictionary update here, may be\n",
    "    #merged_tokens.append(split_text)\n",
    "\n",
    "    return split_text#sum(splits, [])\n",
    "\n",
    "N=[i for i in range(20000)]\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    # Apply the process_item function to each item in the list\n",
    "    # The map method distributes the workload across multiple processes\n",
    "    mergedParallel = pool.map(tokenizeSinlge, N)\n",
    "\n",
    "\n",
    "#print(merged_tokens==mergedParallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mergedParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_str = str(bpe)\n",
    "\n",
    "with open('./TXT_ast_augmented_new.txt', 'w') as file:\n",
    "    file.write(bpe_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
